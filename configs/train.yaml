# Defaults
defaults:
  - dataloaders: kmeans
  - model: mlp
  - loss: mse
  - _self_

# Device and precision
device: mps          # choices: cpu, cuda, mps (on Apple)
precision: fp32       # choices: fp32, bf16, fp16
compile: false        # torch.compile the model (may speed up training)

# Training
epochs: 10
eval_every: 1
gradient_accumulation_steps: 1  # Accumulate gradients over N batches for larger effective batch size

# Checkpointing
ckpt_dir: outputs/ckpts
ckpt_name: last.pt
resume: false
save_every: 1   # save every N epochs
eval_checkpoint: null

# Optimizer (Hydra-instantiable)
optimizer:
  _target_: torch.optim.AdamW
  lr: 1.0e-3
  weight_decay: 0.0

# Solver configuration
solver:
  _target_: solvers.euler_solver.EulerSolver
  num_steps: 50

# Run name (required) - identifies this training run
run_name: ???  # Must be provided via CLI: python launcher.py run_name=my_experiment

# Experiment tracking (Trackio)
trackio:
  enabled: true
  project: denoising-zoo
  log_every: 10  # log train metrics every N batches

